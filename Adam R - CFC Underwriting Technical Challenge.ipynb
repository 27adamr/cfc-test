{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CFC Insight Technical Challenge\n",
    "### Adam Richmond - December 2022\n",
    "\n",
    "\n",
    "I wrote this challenge as a Jupyter notebook in order to maximise the presentability and readability of the code. I think this will also make it easier both to discuss in a follow-up interview and to see the logical flow of how I approached the problem. \n",
    "\n",
    "## Section 1\n",
    "_Scrape the index webpage hosted at `cfcunderwriting.com`_\n",
    "\n",
    "I'll use the python `requests` package to get the data from the website, and the `BeautifulSoup` package for parsing HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# get page data\n",
    "index_page = requests.get(\"https://www.cfcunderwriting.com/en-gb/\")\n",
    "\n",
    "# parse HTML\n",
    "index_soup = BeautifulSoup(index_page.content, \"html.parser\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence the HTML content of the index web page is displayed in a readable format. Additionally, using a Jupyter notebook means the page's content is cached in the `index_soup` variable and available in other cells, so that unnecessary GET requests to the site can be avoided.\n",
    "\n",
    "## Section 2\n",
    "_Write a list of all externally loaded resources (e.g. images/scripts/fonts not hosted\n",
    "on cfcunderwriting.com) to a JSON output file_\n",
    "\n",
    "My strategy for this is as follows:\n",
    "1. Find all of the HTML tags typically associated with external resources, such as img or script\n",
    "2. Find the \"src\" attribute of these tags and check it's external - if so, add it to a dictionary\n",
    "3. Convert the dictionary to JSON and write it to a file.\n",
    "\n",
    "By inspecting the above there are a number of resources of various types used in the site, but not all are externally hosted. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"img\": [], \"script\": [\"//js.hsforms.net/forms/v2.js\", \"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js\", \"https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/4.1.3/js/bootstrap.min.js\", \"//js.hs-scripts.com/6072523.js\", \"//js.hsforms.net/forms/v2.js\", \"https://www.google.com/recaptcha/api.js?render=6LemiyEaAAAAAGwb4nR8oX38fxyM36xjIGbwz6d4\"], \"font\": [], \"iframe\": [\"https://www.googletagmanager.com/ns.html?id=GTM-NGGN5FB\"]}\n"
     ]
    }
   ],
   "source": [
    "# import libraries: regex to search text and json handling for output\n",
    "import re\n",
    "import json\n",
    "\n",
    "\n",
    "# define a list of HTML tags usually associated with external resources\n",
    "external_tag_types = [\"img\", \"script\", \"font\", \"iframe\"]\n",
    "\n",
    "\n",
    "def find_all_sources(soup, external_tags):\n",
    "    \"\"\"\n",
    "    Given a list of tag types, finds all externally sourced instances of each tag. \n",
    "    \"\"\"\n",
    "    output = {key: [] for key in external_tags}\n",
    "    # for each type of tag, find all instances, then for each instance, get its source, and keep if external\n",
    "    for tag_type in external_tag_types:\n",
    "        tags = soup.find_all(tag_type)\n",
    "        for tag in tags:\n",
    "            source = get_source_of_external_tag(tag)\n",
    "            if source is not None:\n",
    "                output[tag_type].append(source)\n",
    "    return output\n",
    "\n",
    "\n",
    "                \n",
    "def get_source_of_external_tag(tag):\n",
    "    \"\"\"\n",
    "    Given a tag, returns its source. \\\n",
    "    Either by finding (\"src\") attribute, or a \"src\" declaration in innerHTML. \\\n",
    "    Returns None if tag's source is internal\n",
    "    \"\"\"\n",
    "    source = None\n",
    "    # check which attributes exist in the tag that indicate its source\n",
    "    attributes = list(tag.attrs)\n",
    "    if \"src\" in attributes:\n",
    "        source = tag[\"src\"]\n",
    "    # if a script has a \"src\" declaration in its innerHTML, find that too (using regex for \"src=\")\n",
    "    elif \"src=\" in tag.text:\n",
    "        regex_search = re.search(r\"(src=\\\")([^\\\"]*)\", tag.text)\n",
    "        if regex_search:\n",
    "            source = regex_search.group(2)\n",
    "    if source:\n",
    "        if not source_is_external(source):  # return None if source is internal\n",
    "            source = None\n",
    "    return source\n",
    "\n",
    "\n",
    "def source_is_external(source):\n",
    "    \"\"\"\n",
    "    Given a source, returns a boolean representing if the tag is internal or external based on its source\n",
    "    \"\"\"\n",
    "    is_external = False\n",
    "    if source.startswith(\"//\"):  # external resources are addressed by double slash, internal by single\n",
    "        is_external = True\n",
    "    elif (source.startswith(\"http\")) and (\"www.cfcunderwriting.com\" not in source):  # if http, check not internal\n",
    "        is_external = True\n",
    "    return is_external\n",
    "\n",
    "    \n",
    "def write_file(output):\n",
    "    \"\"\"\n",
    "    Writes the external source data to a file\n",
    "    \"\"\"\n",
    "    with open(\"external_sources.json\", \"w\") as f:\n",
    "        f.write(output)\n",
    "    \n",
    "sources = find_all_sources(index_soup, external_tag_types)\n",
    "write_file(json.dumps(sources))\n",
    "print(json.dumps(sources))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output is written in the `external_sources.json` file.\n",
    "\n",
    "## Section 3\n",
    "_Enumerate the page's hyperlinks and identify the location of the \"Privacy Policy\" page._\n",
    "\n",
    "To do this I simply find all `a` tags and check the tag's text to see if \"privacy policy\" is mentioned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<a href=\"/en-gb/support/privacy-policy/\">Privacy policy</a>, <a data-udi=\"umb://document/b8c2aeeef41e4e73812afab207e45b89\" href=\"/en-gb/support/privacy-policy/\" title=\"Privacy policy\">Privacy Policy</a>]\n"
     ]
    }
   ],
   "source": [
    "# find all links - denoted by HTML a-tag\n",
    "links = index_soup.find_all(\"a\")\n",
    "\n",
    "# print if the tag's contents mention privacy policy\n",
    "print([link for link in links if \"privacy policy\" in link.text.lower()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The link is therefore `base_url/en-gb/support/privacy-policy`, where `base_url` is www.cfcunderwriting.com.\n",
    "\n",
    "## Section 4\n",
    "_Use the privacy policy URL identified in step 3 and scrape the pages content.\n",
    "Produce a case-insentitive word frequency count for all of the visible text on the page.\n",
    "Your frequency count should also be written to a JSON output file_\n",
    "\n",
    "I'll scrape and parse the data from this page as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get page data\n",
    "privacy_page = requests.get(\"https://www.cfcunderwriting.com/en-gb/support/privacy-policy\")\n",
    "\n",
    "# parse HTML\n",
    "privacy_soup = BeautifulSoup(index_page.content, \"html.parser\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My strategy for finding all visible words on the page is as follows:\n",
    "1. Find all child tags of the main \"body\" tag\n",
    "2. Check these tags don't have their \"style.visibility\" attribute set to \"hidden\"\n",
    "3. Find all of the words in the tag's innerHTML, cleaning up newlines as I go, to produce a space-separated list of words\n",
    "4. Split this list of words by the space character and count its length to get the word count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'word_count': 1092}\n"
     ]
    }
   ],
   "source": [
    "def get_words_from_tag(tag):\n",
    "    \"\"\"\n",
    "    Finds all of the words in a div - separated by a space - and returns them in a list. \\\n",
    "    Returns \"\" if tag is invisible\n",
    "    \"\"\"\n",
    "    # first check visibility is not hidden\n",
    "    try:\n",
    "        invisible = \"visibility: hidden\" in tag[\"style\"]\n",
    "    except Exception:\n",
    "        invisible = False\n",
    "    # find words in div and reformat\n",
    "    if invisible:\n",
    "        return \"\"\n",
    "    else:\n",
    "        words = replace_punctuation(tag.text)\n",
    "        return words\n",
    "\n",
    "def replace_punctuation(text):\n",
    "    \"\"\" \n",
    "    Replaces newlines and punctuations\n",
    "    \"\"\"\n",
    "    if \"\\n\" in text:  # replace newlines\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "# find all tags inside <body> and get their innerHTML contents unless they are hidden\n",
    "text_list = []\n",
    "body_tag = privacy_soup.find(\"body\")\n",
    "for child_tag in body_tag:\n",
    "    words = get_words_from_tag(child_tag)\n",
    "    if words != \"\" and words != \" \":  # don't add if empty\n",
    "        text_list.append(words)\n",
    "\n",
    "# remove any duplicates\n",
    "text_list = list(dict.fromkeys(text_list))\n",
    "\n",
    "# flatten list of entries into list of words\n",
    "word_list = []\n",
    "for text in text_list:\n",
    "    word_list += text.split(\" \")\n",
    "    \n",
    "# finally, drop all empty spaces\n",
    "word_list = list(filter(lambda entry: entry != '', word_list))\n",
    "\n",
    "# find the word count by splitting by space character and counting list length\n",
    "result = {\"word_count\": len(word_list)}\n",
    "with open(\"word_count.json\", \"w\") as f:\n",
    "    f.write(json.dumps(result))\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence, the word count of visible words on the Privacy Policy page is 1,092."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
